{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "In this assignment your goal is to predict the credit application result (not granted / granted) based on multiple features. You'll have to do everything on your own this time, no hints.\n",
    "\n",
    "The dataset contains 15 features (named A1 - A15) and one target variable (T). We don't know what the features are, we only know what values they take:\n",
    "\n",
    "* A1: b, a.\n",
    "* A2: continuous\n",
    "* A3: continuous\n",
    "* A4: u, y, l, t\n",
    "* A5: g, p, gg\n",
    "* A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff\n",
    "* A7: v, h, bb, j, n, z, dd, ff, o\n",
    "* A8: continuous\n",
    "* A9: t, f\n",
    "* A10: t, f\n",
    "* A11: continuous\n",
    "* A12: t, f\n",
    "* A13: g, p, s\n",
    "* A14: continuous\n",
    "* A15: continuous\n",
    "* T: +, -\n",
    "\n",
    "There are 6 features that are continous, 3 true/false variables and 6 categorical variables that take different values each.\n",
    "\n",
    "From the basic diagnostic of the data, you can see it's a real mess. There are different data types, missing observations etc.\n",
    "\n",
    "Your task is to build three different classifiers (and one additional GridSearched) that correctly (as much as possible) predict the credit application decision (this is the target variable 'T'). To achieve this, you'll have to do the following:\n",
    "\n",
    "1. Clean up the dataset - when you read it now, you'll notice that the data is not correctly parsed. The columns that contain numbers are read as 'object' type columns. This is because the missing values in the data set are marked with ?'s.\n",
    "Deal with it by correctly recognizing ?'s as missing values and dropping them from the dataset.\n",
    "2. Encode the features - there are a lot of categorical features with char values. You'll need to use [LabelEncoder and OneHotEncoder](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621) for them. Also, see if you need to use OneHotEncoder for all of them - for example by checking the correlations between a LabelEncoded feature and the target variable before and after OneHot Encoding. (OneHot encoding doesn't make any sense if your variable actually represents some incremental, hieraarchical relationship - and we don't know it for our dataset).\n",
    "3. This brings us to the next point - take a look into the data set: plot the histograms, check the skewness, correlations etc. ([also check this](https://seaborn.pydata.org/generated/seaborn.pairplot.html))\n",
    "4. Train at least 3 diffeent classifiers using cross-validation. There must be at least one *simple* and one ensemble classifier. Compare those classifiers' precissions, recalls, ROC's etc. **Don't forget to use cross-validation.**\n",
    "5. For one selected classifier run Grid Search and, once the best parameter combination is found, compare it's performance metrics with those of classifiers from point 4.\n",
    "\n",
    "Scoring:\n",
    "1. Clean-up: 1 point\n",
    "2. Encoding: 1 point\n",
    "3. Data exploration: 2 points (1 point for corellations, histograms, etc., 1 for extras such as attempts at dimensionality reduction or non-linear correlations)\n",
    "4. Training the 3 classifiers: 1 point for each (3 points total), 1 point for meaningful comparison.\n",
    "5. Grid Search: 1 point for setting up and training, 1 point for comparing with previous classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "column_names = [f\"A{i}\" for i in range(1, 16)]\n",
    "column_names.append('T')\n",
    "\n",
    "data = read_csv('data.csv', names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1      object\n",
      "A2      object\n",
      "A3     float64\n",
      "A4      object\n",
      "A5      object\n",
      "A6      object\n",
      "A7      object\n",
      "A8     float64\n",
      "A9      object\n",
      "A10     object\n",
      "A11      int64\n",
      "A12     object\n",
      "A13     object\n",
      "A14     object\n",
      "A15      int64\n",
      "T       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A1     A2      A3 A4 A5  A6 A7     A8 A9 A10  A11 A12 A13    A14    A15  T\n",
      "0   b  30.83   0.000  u  g   w  v  1.250  t   t    1   f   g  00202      0  +\n",
      "1   a  58.67   4.460  u  g   q  h  3.040  t   t    6   f   g  00043    560  +\n",
      "2   a  24.50   0.500  u  g   q  h  1.500  t   f    0   f   g  00280    824  +\n",
      "3   b  27.83   1.540  u  g   w  v  3.750  t   t    5   t   g  00100      3  +\n",
      "4   b  20.17   5.625  u  g   w  v  1.710  t   f    0   f   s  00120      0  +\n",
      "5   b  32.08   4.000  u  g   m  v  2.500  t   f    0   t   g  00360      0  +\n",
      "6   b  33.17   1.040  u  g   r  h  6.500  t   f    0   t   g  00164  31285  +\n",
      "7   a  22.92  11.585  u  g  cc  v  0.040  t   f    0   f   g  00080   1349  +\n",
      "8   b  54.42   0.500  y  p   k  h  3.960  t   f    0   f   g  00180    314  +\n",
      "9   b  42.50   4.915  y  p   w  v  3.165  t   f    0   t   g  00052   1442  +\n",
      "10  b  22.08   0.830  u  g   c  h  2.165  f   f    0   t   g  00128      0  +\n",
      "11  b  29.92   1.835  u  g   c  h  4.335  t   f    0   f   g  00260    200  +\n",
      "12  a  38.25   6.000  u  g   k  v  1.000  t   f    0   t   g  00000      0  +\n",
      "13  b  48.08   6.040  u  g   k  v  0.040  f   f    0   f   g  00000   2690  +\n",
      "14  a  45.83  10.500  u  g   q  v  5.000  t   t    7   t   g  00000      0  +\n",
      "15  b  36.67   4.415  y  p   k  v  0.250  t   t   10   t   g  00320      0  +\n",
      "16  b  28.25   0.875  u  g   m  v  0.960  t   t    3   t   g  00396      0  +\n",
      "17  a  23.25   5.875  u  g   q  v  3.170  t   t   10   f   g  00120    245  +\n",
      "18  b  21.83   0.250  u  g   d  h  0.665  t   f    0   t   g  00000      0  +\n",
      "19  a  19.17   8.585  u  g  cc  h  0.750  t   t    7   f   g  00096      0  +\n"
     ]
    }
   ],
   "source": [
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1     0\n",
      "A2     0\n",
      "A3     0\n",
      "A4     0\n",
      "A5     0\n",
      "A6     0\n",
      "A7     0\n",
      "A8     0\n",
      "A9     0\n",
      "A10    0\n",
      "A11    0\n",
      "A12    0\n",
      "A13    0\n",
      "A14    0\n",
      "A15    0\n",
      "T      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# missing values - why is it 0 everywhere?\n",
    "print(data.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
