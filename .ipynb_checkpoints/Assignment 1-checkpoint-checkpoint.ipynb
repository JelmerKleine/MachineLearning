{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "In this assignment you will work with Linear Regression and Gradient Descent. The dataset that you will use is the so-called *Boston house pricing dataset*.\n",
    "## Preparation\n",
    "\n",
    "First we'll load some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston House Pricing\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "In this part of the assignment you will try to predict the prices of houses in Boston. Let's load the data and see what's in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [label.strip() for label in open(\"columns.names\").readline().rstrip().split(',')]\n",
    "bdata = read_csv(\"housing.data\", sep=\"\\s+\", header=None, names=colnames)\n",
    "bdata.head() #take a look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have some data! There are 13 different features in the dataset (from CRIM to LSTAT) and one value that we will try to predict based on the features (MEDV - median house price).\n",
    "What kind of data exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly floats and some ints, now how many data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also good to check if we have any missing data or NaN's (not-a-number) in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bdata.isnull().sum())\n",
    "print(bdata.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No and no - luckily no need to remove observations.\n",
    "\n",
    "Now it's time to look closer into the data and see how it looks like. First, let's use the pandas `describe` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision', 1)\n",
    "bdata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there's lots of basic statistic for each column being printed. However since we are dealing with a regression problem it's far more interesting to see if there are any correlations between the features.\n",
    "We can do it in text mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision', 2)\n",
    "bdata.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the last column we can see the correlations between the various features and the median house prices. Usually, correlations above (absolute value of) 0.5 are 'promissing' when it comes to building regression models. \n",
    "Here we will lower this limit to 0.4 and drop all the columns except: INDUS, NOX, RM, TAX, PTRATIO, LSTAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove all the columns except INDUS, NOX, RM, TAX, PTRATIO, LSTAT, MEDV\n",
    "# make sure to name your new dataframe: clean_data\n",
    "# Score: 1 point\n",
    "# clean_data = \n",
    "\n",
    "# If everything went good your dataset should now contain only the colums: INDUS, NOX, RM, TAX, PTRATIO, LSTAT, MEDV, check it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into the features and a vector of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = clean_data.drop('MEDV', axis = 1)\n",
    "prices = clean_data['MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression and Learning Curves\n",
    "\n",
    "If you look at the data above you will notice that the features have different scales, to use the regression models you'll need to build a pipeline that uses scaling of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a scikit Pipeline that contains the scaling and linear regression\n",
    "# make sure to name this pipeline: lin_regressor\n",
    "# Score: 1 point\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# lin_regressor = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the pipeline build, now it's time to run linear regression:\n",
    "\n",
    "1. Split the dataset into the training and validation sets\n",
    "2. Train the model and see what's the RME on the training and on the validation data is\n",
    "3. (Additionally) Wrap you code into a loop that will plot the learning curves by training your model against the data of different sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data into the training and validation data, train the model, plot the learning curves\n",
    "# make sure that you call your split data sets as below and that you name the predicted values of the training set: y_train_predict\n",
    "# Points 1 point for each item (3 points total)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_val, y_train, y_val = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look that impressive - the RMSE is around 5, which given that most values you are trying to predict are in the 20-30 range gives a prediction error of almost 25%! \n",
    "We can also plot the errors of our predictions (those are called *residuals*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking residuals\n",
    "y_train_predict = lin_regressor.predict(X_train)\n",
    "plt.scatter(y_train_predict, y_train-y_train_predict)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot gives us some hope - most of the errors fall in the +/-5 range except a few outliers - perhaps if we could somehow manipulate and clean the input data the results could be better. What's also curious is the shape of residuals that looks a bit like a quadratic function - perhaps we have some polynominal dependency?\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Let's look what our data actually looks like - this can be done by plotting histograms (or the density functions) of all the features in the dataset.\n",
    "\n",
    "We can either use the Pandas dataframe fucntionality or rely on the seaborn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdata.hist(bins=20,figsize=(12,10),grid=False);\n",
    "\n",
    "# using seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, nrows=4, figsize=(12, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k, v in bdata.items():\n",
    "    sns.distplot(v, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, those are the same plots, the seaborn-generated ones are a bit nicer but that's the selling point of seaborn.\n",
    "What you will notice is that our input data looks just awful. Only RM has a nice normal distribution, the rest not so much. We see exponential distributions (e.g. NOX, DIS), bimodal distributions (e.g. RAD, INDUS), weird peaks in data (e.g. ZN) and so on. This is all bad for Linear Regression which we are trying to use here - Linear Regression works the best with normally distributed data. Let's see if we can fix it somehow. We'll start by transforming features that are exponentially distributed - those are CRIM, NOX, DIS and LSTAT. To get rid of the exponentiation you need to take a logarithm - e.g for LSTAT the result looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log(bdata['LSTAT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better than the original!\n",
    "Instead of doing transformations one feature by one we are going to create a new dataframe with the exponential columns transformed. For this we will use the [`assign`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that we create a new data frame here by replacing columns in bdata\n",
    "#normally it's better to create a data transformer (by using ColumnTransformer and FunctionTransformer in this case.)\n",
    "new_data = bdata.assign(CRIM=lambda x: np.log(x.CRIM),\n",
    "                       NOX = lambda x: np.log(x.NOX),\n",
    "                        DIS = lambda x: np.log(x.DIS),\n",
    "                        LSTAT = lambda x: np.log(x.LSTAT))\n",
    "\n",
    "#plot the resulting distributions\n",
    "fig, axs = plt.subplots(ncols=4, nrows=4, figsize=(12, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k, v in new_data.items():\n",
    "    sns.distplot(v, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the distributions for CRIM, NOX, DIS and LSTAT look less skewed now.\n",
    "\n",
    "### Outliers\n",
    "\n",
    "Now we are going to try to remove outliers - those are the observations that are far away from other observations. The easy way to check for outliers in our feature set is by using boxplots: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k, v in new_data.items():\n",
    "    sns.boxplot(y=k, data=new_data, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will notice when looking at the plots is that the features ZN, RM and B have many outliers, what's even worse the value we are trying to predict MEDV also has some! We are not going to remove the outliers from the features but we definitely need to get rid of those in MEDV - if you look at the distribution you'll notice that there are a couple of values that are exactly 50 - those are most likely data injected into the set when no real price was available or when it was higher than 50 - let's remove those observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 5))\n",
    "axs.flatten()\n",
    "sns.distplot(new_data['MEDV'], ax = axs[0])\n",
    "new_data = new_data[(new_data['MEDV'] != 50)]\n",
    "sns.distplot(new_data['MEDV'], ax = axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collinearity\n",
    "\n",
    "The last thing we are going to do is to look at the Collinearity of the features - this is checking whether some features are strongly corellated. Such features shouldn't be used together in the Linear Regression. We are going to look again at the correlations but this time using the [`heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function of seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = new_data.corr().abs()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr, square=True, annot=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a value of 0.8 as a treshold, the following features are highly collinear:\n",
    "CRIM: TAX, RAD, NOX\n",
    "NOX: TAX, DIS, CRIM\n",
    "DIS: NOX\n",
    "RAD: TAX, CRIM\n",
    "TAX: RAD, CRIM\n",
    "\n",
    "On the other, taking 0.5 as the limit MEDV seems to be correlated with LSTAT (0.82), PTRATIO (0.52), TAX (0.57), RM (0.69), NOX (0.53), INDUS (0.6), CRIM (0.57).\n",
    "\n",
    "In the final feature selection for Linear Regression we will use LSTAT, PTRATIO, RM, INDUS and TAX. Neither CRIM nor NOX make it because of the collinearity with TAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = new_data['MEDV']\n",
    "features = new_data[['LSTAT', 'PTRATIO', 'RM', 'INDUS', 'TAX']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the features and the values to predict cleaned up and selected your taks is as follows:\n",
    "\n",
    "1. Build a processing pipeline that includes: addition of polynominal features, feature scaler and a regularized regressor (Linear won't do for poly features) \\[1 point\\]\n",
    "\n",
    "2. Split the dataset (new_data) into the training and validation sets and plot the learning curves \\[1 point\\]\n",
    "\n",
    "3. Build at least two additional pipelines:\n",
    "\n",
    "    a) one that includes polynominal features and `LinearRegression`\n",
    "    \n",
    "    b) one that includes polynominal features and another kind of regularized Regressor\n",
    "    \n",
    "    c) compare the performance of those three approaches by comparing cross-validation scores using the k-fold strategy  \\[3 points\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import ???\n",
    "# from sklearn.preprocessing import\n",
    "\n",
    "# TODO: Build the first pipeline (1 point)\n",
    "# TODO: Split the dataset and plot the learning curves (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import \n",
    "# TODO: Build the additional pipelines, plot learning curves and use cross-validation to compare the regressors (3 points)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
